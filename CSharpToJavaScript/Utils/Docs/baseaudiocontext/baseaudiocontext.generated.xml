<docs>
<BaseAudioContext>
<summary>
The <c>BaseAudioContext</c> interface of the <see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API">Web Audio API</see> acts as a base definition for online and offline audio-processing graphs, as represented by <see cref="AudioContext"/> and <see cref="OfflineAudioContext"/> respectively. You wouldn't use <c>BaseAudioContext</c> directly â€” you&amp;apos;d use its features via one of these two inheriting interfaces.
</summary>
<remarks>
<para>A <c>BaseAudioContext</c> can be a target of events, therefore it implements the <see cref="EventTarget"/> interface.</para><para></para>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/>-<see cref="AudioContext"/><br/>-<see cref="OfflineAudioContext"/><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext"> <em>See also on MDN</em> </seealso></para>
</remarks>
</BaseAudioContext>
<BaseAudioContextCreateOscillator>
<summary>
The <c>createOscillator()</c> method of the <see cref="BaseAudioContext"/><br/>interface creates an <see cref="OscillatorNode"/>, a source representing a periodic<br/>waveform. It basically generates a constant tone.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="OscillatorNode.OscillatorNode"/><br/>constructor is the recommended way to create a <see cref="OscillatorNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createOscillator"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>An <see cref="OscillatorNode"/>.</returns>
</BaseAudioContextCreateOscillator>
<BaseAudioContextCreateAnalyser>
<summary>
The <c>createAnalyser()</c> method of the<br/><see cref="BaseAudioContext"/> interface creates an <see cref="AnalyserNode"/>, which<br/>can be used to expose audio time and frequency data and create data visualizations.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="AnalyserNode.AnalyserNode"/> constructor is the<br/>recommended way to create an <see cref="AnalyserNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote><blockquote class="NOTE"><h5>NOTE</h5><para>For more on using this node, see the<br/><see cref="AnalyserNode"/> page.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createAnalyser"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>An <see cref="AnalyserNode"/>.</returns>
</BaseAudioContextCreateAnalyser>
<BaseAudioContextCreateConvolver>
<summary>
The <c>createConvolver()</c> method of the <see cref="BaseAudioContext"/><br/>interface creates a <see cref="ConvolverNode"/>, which is commonly used to apply<br/>reverb effects to your audio. See the <see href="https://webaudio.github.io/web-audio-api/#background-3">spec definition of Convolution</see> for more information.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="ConvolverNode.ConvolverNode"/><br/>constructor is the recommended way to create a <see cref="ConvolverNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createConvolver"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="ConvolverNode"/>.</returns>
</BaseAudioContextCreateConvolver>
<BaseAudioContextCurrentTime>
<summary>
The <c>currentTime</c> read-only property of the <see cref="BaseAudioContext"/><br/>interface returns a double representing an ever-increasing hardware timestamp in seconds that<br/>can be used for scheduling audio playback, visualizing timelines, etc. It starts at 0.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/currentTime"> <em>See also on MDN</em> </seealso></para>
</remarks>
<value>A floating point number.</value>
</BaseAudioContextCurrentTime>
<BaseAudioContextAudioWorklet>
<summary>
The <c>audioWorklet</c> read-only property of the<br/><see cref="BaseAudioContext"/> interface returns an instance of<br/><see cref="AudioWorklet"/> that can be used for adding<br/><see cref="AudioWorkletProcessor"/>-derived classes which implement custom audio<br/>processing.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using Web Audio API</see><br/>-<see cref="AudioWorkletNode"/><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/audioWorklet"> <em>See also on MDN</em> </seealso></para>
</remarks>
<value>An <see cref="AudioWorklet"/> instance.</value>
</BaseAudioContextAudioWorklet>
<BaseAudioContextCreateIIRFilter>
<summary>
The <strong><c>createIIRFilter()</c></strong> method of the <see cref="BaseAudioContext"/> interface creates an <see cref="IIRFilterNode"/>, which represents a general <strong><see href="https://en.wikipedia.org/wiki/Infinite_impulse_response">infinite impulse response</see></strong> (IIR) filter which can be configured to serve as various types of filter.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="IIRFilterNode.IIRFilterNode"/><br/>constructor is the recommended way to create a <see cref="IIRFilterNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/>-<see cref="IIRFilterNode"/><br/>-<see cref="AudioNode"/><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createIIRFilter"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>An <see cref="IIRFilterNode"/> implementing the filter with the specified feedback and<br/>feedforward coefficient arrays.</returns>
</BaseAudioContextCreateIIRFilter>
<BaseAudioContextDecodeAudioData>
<summary>
The <c>decodeAudioData()</c> method of the <see cref="BaseAudioContext"/><br/>Interface is used to asynchronously decode audio file data contained in an<br/>{{jsxref("ArrayBuffer")}} that is loaded from <see cref="Windowfetch"/>,<br/><see cref="XMLHttpRequest"/>, or <see cref="FileReader"/>. The decoded<br/><see cref="AudioBuffer"/> is resampled to the <see cref="AudioContext"/>'s sampling<br/>rate, then passed to a callback or promise.
</summary>
<remarks>
<para>This is the preferred method of creating an audio source for Web Audio API from an<br/>audio track. This method only works on complete file data, not fragments of audio file<br/>data.</para><para>This function implements two alternative ways to asynchronously return the audio data or error messages: it returns a <see cref="Promise"/> that fulfills with the audio data, and also accepts callback arguments to handle success or failure. The primary method of interfacing with this function is via its Promise return value, and the callback parameters are provided for legacy reasons.</para>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/decodeAudioData"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="Promise"/> object that fulfills with the <strong>decodedData</strong>. If you are using the<br/>XHR syntax you will ignore this return value and use a callback function instead.</returns>
</BaseAudioContextDecodeAudioData>
<BaseAudioContextCreateConstantSource>
<summary>
The <strong><c>createConstantSource()</c></strong><br/>property of the <see cref="BaseAudioContext"/> interface creates a<br/><see cref="ConstantSourceNode"/> object, which is an audio source that continuously<br/>outputs a monaural (one-channel) sound signal whose samples all have the same<br/>value.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="ConstantSourceNode.ConstantSourceNode"/><br/>constructor is the recommended way to create a <see cref="ConstantSourceNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createConstantSource"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="'ConstantSourceNode'"/> instance.</returns>
</BaseAudioContextCreateConstantSource>
<BaseAudioContextCreateBuffer>
<summary>
The <c>createBuffer()</c> method of the <see cref="BaseAudioContext"/><br/>Interface is used to create a new, empty <see cref="AudioBuffer"/> object, which<br/>can then be populated by data, and played via an <see cref="AudioBufferSourceNode"/>.
</summary>
<remarks>
<para>For more details about audio buffers, check out the <see cref="AudioBuffer"/><br/>reference page.</para><blockquote class="NOTE"><h5>NOTE</h5><para><c>createBuffer()</c> used to be able to take compressed<br/>data and give back decoded samples, but this ability was removed from the specification,<br/>because all the decoding was done on the main thread, so<br/><c>createBuffer()</c> was blocking other code execution. The asynchronous method<br/><c>decodeAudioData()</c> does the same thing â€” takes compressed audio, such as an<br/>MP3 file, and directly gives you back an <see cref="AudioBuffer"/> that you can<br/>then play via an <see cref="AudioBufferSourceNode"/>. For simple use cases<br/>like playing an MP3, <c>decodeAudioData()</c> is what you should be using.</para></blockquote><para>For an in-depth explanation of how audio buffers work, including what the parameters do, read <see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API#audio_buffers_frames_samples_and_channels">Audio buffers: frames, samples and channels</see> from our Basic concepts guide.</para>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createBuffer"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>An <see cref="AudioBuffer"/> configured based on the specified options.</returns>
</BaseAudioContextCreateBuffer>
<BaseAudioContextCreateChannelSplitter>
<summary>
The <c>createChannelSplitter()</c> method of the <see cref="BaseAudioContext"/> Interface is used to create a <see cref="ChannelSplitterNode"/>,<br/>which is used to access the individual channels of an audio stream and process them separately.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="ChannelSplitterNode.ChannelSplitterNode"/><br/>constructor is the recommended way to create a <see cref="ChannelSplitterNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createChannelSplitter"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="ChannelSplitterNode"/>.</returns>
</BaseAudioContextCreateChannelSplitter>
<BaseAudioContextCreateWaveShaper>
<summary>
The <c>createWaveShaper()</c> method of the <see cref="BaseAudioContext"/><br/>interface creates a <see cref="WaveShaperNode"/>, which represents a non-linear<br/>distortion. It is used to apply distortion effects to your audio.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="WaveShaperNode.WaveShaperNode"/><br/>constructor is the recommended way to create a <see cref="WaveShaperNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createWaveShaper"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="WaveShaperNode"/>.</returns>
</BaseAudioContextCreateWaveShaper>
<BaseAudioContextCreatePeriodicWave>
<summary>
The <c>createPeriodicWave()</c> method of the <see cref="BaseAudioContext"/> interface is used to create a <see cref="PeriodicWave"/>. This wave is used to define a periodic waveform that can be used to shape the output of an <see cref="OscillatorNode"/>.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createPeriodicWave"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="PeriodicWave"/>.</returns>
</BaseAudioContextCreatePeriodicWave>
<BaseAudioContextListener>
<summary>
The <c>listener</c> property of the <see cref="BaseAudioContext"/> interface<br/>returns an <see cref="AudioListener"/> object that can then be used for<br/>implementing 3D audio spatialization.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/listener"> <em>See also on MDN</em> </seealso></para>
</remarks>
<value>An <see cref="AudioListener"/> object.</value>
</BaseAudioContextListener>
<BaseAudioContextState>
<summary>
The <c>state</c> read-only property of the <see cref="BaseAudioContext"/><br/>interface returns the current state of the <c>AudioContext</c>.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/state"> <em>See also on MDN</em> </seealso></para>
</remarks>
<value>A string. Possible values are:</value>
</BaseAudioContextState>
<BaseAudioContextCreateScriptProcessor>
<summary>
<div class="IMPORTANT"><h5>IMPORTANT</h5> <strong>Deprecated</strong></div> The <c>createScriptProcessor()</c> method of the <see cref="BaseAudioContext"/> interface<br/>creates a <see cref="ScriptProcessorNode"/> used for direct audio processing.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>This feature was replaced by <see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet">AudioWorklets</see> and the <see cref="AudioWorkletNode"/> interface.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createScriptProcessor"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="ScriptProcessorNode"/>.</returns>
</BaseAudioContextCreateScriptProcessor>
<BaseAudioContextCreateDelay>
<summary>
The <c>createDelay()</c> method of the<br/><see cref="BaseAudioContext"/> Interface is used to create a <see cref="DelayNode"/>,<br/>which is used to delay the incoming audio signal by a certain amount of time.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="DelayNode.DelayNode"/><br/>constructor is the recommended way to create a <see cref="DelayNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createDelay"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="DelayNode"/>. The default <see cref="DelayNode.DelayTime"/> is 0<br/>seconds.</returns>
</BaseAudioContextCreateDelay>
<BaseAudioContextCreateGain>
<summary>
The <c>createGain()</c> method of the <see cref="BaseAudioContext"/><br/>interface creates a <see cref="GainNode"/>, which can be used to control the<br/>overall gain (or volume) of the audio graph.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="GainNode.GainNode"/><br/>constructor is the recommended way to create a <see cref="GainNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createGain"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="GainNode"/> which takes as input one or more audio sources and outputs<br/>audio whose volume has been adjusted in gain (volume) to a level specified by the node&amp;apos;s<br/><see cref="GainNode.Gain"/> <see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioParam#a-rate">a-rate</see><br/>parameter.</returns>
</BaseAudioContextCreateGain>
<BaseAudioContextCreateDynamicsCompressor>
<summary>
The <c>createDynamicsCompressor()</c> method of the <see cref="BaseAudioContext"/> Interface is used to create a <see cref="DynamicsCompressorNode"/>, which can be used to apply compression to an audio signal.
</summary>
<remarks>
<para>Compression lowers the volume of the loudest parts of the signal and raises the volume<br/>of the softest parts. Overall, a louder, richer, and fuller sound can be achieved. It is<br/>especially important in games and musical applications where large numbers of individual<br/>sounds are played simultaneously, where you want to control the overall signal level and<br/>help avoid clipping (distorting) of the audio output.</para><blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="DynamicsCompressorNode.DynamicsCompressorNode"/><br/>constructor is the recommended way to create a <see cref="DynamicsCompressorNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createDynamicsCompressor"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="DynamicsCompressorNode"/>.</returns>
</BaseAudioContextCreateDynamicsCompressor>
<BaseAudioContextStatechange>
<summary>
A <c>statechange</c> event is fired at a <see cref="BaseAudioContext"/> object when its <see cref="BaseAudioContext.State"/> member changes.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/statechange"> <em>See also on MDN</em> </seealso></para>
</remarks>
</BaseAudioContextStatechange>
<BaseAudioContextCreateBufferSource>
<summary>
The <c>createBufferSource()</c> method of the <see cref="BaseAudioContext"/><br/>Interface is used to create a new <see cref="AudioBufferSourceNode"/>, which can be<br/>used to play audio data contained within an <see cref="AudioBuffer"/> object.<br/><see cref="AudioBuffer"/>s are created using <see cref="BaseAudioContext.CreateBuffer"/> or returned by <see cref="BaseAudioContext.DecodeAudioData"/> when it successfully decodes an audio track.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="AudioBufferSourceNode.AudioBufferSourceNode"/><br/>constructor is the recommended way to create a <see cref="AudioBufferSourceNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createBufferSource"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>An <see cref="AudioBufferSourceNode"/>.</returns>
</BaseAudioContextCreateBufferSource>
<BaseAudioContextDestination>
<summary>
The <c>destination</c> property of the <see cref="BaseAudioContext"/><br/>interface returns an <see cref="AudioDestinationNode"/> representing the final<br/>destination of all audio in the context. It often represents an actual audio-rendering<br/>device such as your device&amp;apos;s speakers.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/destination"> <em>See also on MDN</em> </seealso></para>
</remarks>
<value>An <see cref="AudioDestinationNode"/>.</value>
</BaseAudioContextDestination>
<BaseAudioContextSampleRate>
<summary>
The <c>sampleRate</c> property of the <see cref="BaseAudioContext"/> interface returns a floating point number representing the sample rate, in samples per second, used by all nodes in this audio context.<br/>This limitation means that sample-rate converters are not supported.
</summary>
<remarks>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/sampleRate"> <em>See also on MDN</em> </seealso></para>
</remarks>
<value>A floating point number indicating the audio context&amp;apos;s sample rate, in samples per<br/>second.</value>
</BaseAudioContextSampleRate>
<BaseAudioContextCreatePanner>
<summary>
The <c>createPanner()</c> method of the <see cref="BaseAudioContext"/><br/>Interface is used to create a new <see cref="PannerNode"/>, which is used to<br/>spatialize an incoming audio stream in 3D space.
</summary>
<remarks>
<para>The panner node is spatialized in relation to the AudioContext&amp;apos;s<br/><see cref="AudioListener"/> (defined by the <see cref="BaseAudioContextlistener"/><br/>attribute), which represents the position and orientation of the person listening to the<br/>audio.</para><blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="PannerNode.PannerNode"/><br/>constructor is the recommended way to create a <see cref="PannerNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createPanner"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="PannerNode"/>.</returns>
</BaseAudioContextCreatePanner>
<BaseAudioContextCreateBiquadFilter>
<summary>
The <c>createBiquadFilter()</c> method of the <see cref="BaseAudioContext"/><br/>interface creates a <see cref="BiquadFilterNode"/>, which represents a second order<br/>filter configurable as several different common filter types.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="BiquadFilterNode.BiquadFilterNode"/> constructor is the<br/>recommended way to create a <see cref="BiquadFilterNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createBiquadFilter"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="BiquadFilterNode"/>.</returns>
</BaseAudioContextCreateBiquadFilter>
<BaseAudioContextCreateStereoPanner>
<summary>
The <c>createStereoPanner()</c> method of the <see cref="BaseAudioContext"/> interface creates a <see cref="StereoPannerNode"/>, which can be used to apply<br/>stereo panning to an audio source.<br/>It positions an incoming audio stream in a stereo image using a <see href="https://webaudio.github.io/web-audio-api/#stereopanner-algorithm">low-cost panning algorithm</see>.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="StereoPannerNode.StereoPannerNode"/><br/>constructor is the recommended way to create a <see cref="StereoPannerNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createStereoPanner"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="StereoPannerNode"/>.</returns>
</BaseAudioContextCreateStereoPanner>
<BaseAudioContextCreateChannelMerger>
<summary>
The <c>createChannelMerger()</c> method of the <see cref="BaseAudioContext"/> interface creates a <see cref="ChannelMergerNode"/>,<br/>which combines channels from multiple audio streams into a single audio stream.
</summary>
<remarks>
<blockquote class="NOTE"><h5>NOTE</h5><para>The <see cref="ChannelMergerNode.ChannelMergerNode"/> constructor is the<br/>recommended way to create a <see cref="ChannelMergerNode"/>; see<br/><see href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode#creating_an_audionode">Creating an AudioNode</see>.</para></blockquote>
<para>-<see href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</see><br/></para>
<para><seealso href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createChannelMerger"> <em>See also on MDN</em> </seealso></para>
</remarks>
<returns>A <see cref="ChannelMergerNode"/>.</returns>
</BaseAudioContextCreateChannelMerger>
</docs>